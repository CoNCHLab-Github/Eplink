{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Config parameters\n",
    "root = \"/home/ali/graham-akhanf/EpLink/Eplink\"\n",
    "local_root = \"/home/ali/Workspace/EpLink/Eplink\"\n",
    "\n",
    "atlas = \"none\"\n",
    "atlases_aliases = [\"Vertex\", \"Desikan\", \"Glasser 2016\", \"Schaefer 2018\", \"Yan 2023\"]\n",
    "atlases_path = os.path.join(local_root,\"ISC-pipeline\",\"resources\",\"atlases_fsLR_32K\",'cleaned_versions')\n",
    "\n",
    "dataset = \"eplink-p3\"\n",
    "task = \"movie\"\n",
    "resampled = \"N\"\n",
    "fwhms= [0, 5, 10]\n",
    "confounds_idx = 1\n",
    "\n",
    "results_path = os.path.join(root,\"ISC-pipeline\",\"results\",f\"{dataset}\", \"pwISC\")\n",
    "file_pattern = os.path.join(results_path,f\"pwISC_task-{task}_hemi-{{hemi}}_fwhm-{{fwhm}}_confounds-{confounds_idx}_resampled-{resampled}_atlas-{atlas}.h5\")\n",
    "#bootstrap_file_pattern = \"pwISC_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{atlas}_SWB-pvalues(fdr).h5\"\n",
    "\n",
    "\n",
    "# Participants to exclude\n",
    "subjid_exclude = [68, 75, 76, 79, 80, 84, 86, 90, 91, 94, 5089, 5092, 5105,\n",
    "                  7, 9, 10, 12, 18, 21, 22, 32, 35, 38, 5210, 5212, 5217, 5218, 5219, 5222, 5223, 5225, 5226, 5227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import errno\n",
    "import glob\n",
    "import importlib\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import false_discovery_control as fdr\n",
    "\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, local_root)\n",
    "sys.path.insert(2, os.path.join(local_root, \"ISC-pipeline\"))\n",
    "\n",
    "from utils.Surf import roi2gii, vertex2gii\n",
    "import utils.Vis\n",
    "importlib.reload(utils.Vis)\n",
    "from utils.Vis import plot_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject-wise Bootstrapping (SWB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from workflow.scripts.utils import recon_lt_matrix, vectorize_lt_matrix\n",
    "\n",
    "def bootstrap_pw_matrix(pw_matrix, random_seed=None):\n",
    "    '''This function shuffles subjects within a similarity matrix based on recommendation by Chen et al., 2016'''\n",
    "    random_state = check_random_state(random_seed)\n",
    "    \n",
    "    n_sub = pw_matrix.shape[0]\n",
    "    bootstrap_subject = sorted(random_state.choice(np.arange(n_sub), size=n_sub, replace=True))\n",
    "    return pw_matrix[bootstrap_subject, :][:, bootstrap_subject]\n",
    "\n",
    "def _load_pwISC(fpath):\n",
    "    decode = lambda b: b.decode(\"utf-8\")\n",
    "    decode_np = np.vectorize(decode)\n",
    "\n",
    "    if not os.path.isfile(fpath):\n",
    "        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), fpath)\n",
    "\n",
    "    with h5py.File(fpath, 'r') as f:\n",
    "        # Load the pairwise ISCs\n",
    "        pw_ISC_raw = f['pw_ISC'][:]\n",
    "\n",
    "        # Check if it's saved as a vector or matrix\n",
    "        if len(pw_ISC_raw.shape) <= 2:\n",
    "            # Convert the vectors to matrix\n",
    "            n_units = pw_ISC_raw.shape[0]\n",
    "            pw_ISC = []\n",
    "            for u in range(n_units):\n",
    "                pw_ISC.append(recon_lt_matrix(pw_ISC_raw[u]))\n",
    "            pw_ISC = np.stack(pw_ISC)\n",
    "        else:\n",
    "            pw_ISC = pw_ISC_raw\n",
    "        # Load subjects associated with the matrix\n",
    "        subjects = decode_np(f['subjects'][:])\n",
    "    return pw_ISC, subjects\n",
    "\n",
    "def load_pwISC(fpath):\n",
    "    ISC = []\n",
    "    for h in ['L', 'R']:\n",
    "        isc, subjects = _load_pwISC(fpath.format(hemi=h))\n",
    "        ISC.append(isc)\n",
    "    \n",
    "    return np.concatenate(ISC, axis=0), subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64984/64984 [4:01:35<00:00,  4.48it/s]  \n",
      "100%|██████████| 64984/64984 [4:01:49<00:00,  4.48it/s]  \n",
      "100%|██████████| 64984/64984 [4:01:15<00:00,  4.49it/s]  \n"
     ]
    }
   ],
   "source": [
    "n_perm = 10000\n",
    "output_path = './eplink-p3/stats/'\n",
    "\n",
    "for fwhm in fwhms:\n",
    "    pw_ISC, subjects = load_pwISC(file_pattern.format(fwhm=fwhm,hemi='{hemi}'))\n",
    "    # filter the controls subjects\n",
    "    subjects = np.array(list(map(int, subjects)))\n",
    "    controls = subjects>5000\n",
    "    controls[np.isin(subjects, subjid_exclude)] = False\n",
    "\n",
    "    pw_ISC = pw_ISC[:,controls,:][:,:,controls]\n",
    "    pw_ISC[np.isnan(pw_ISC)] = 0\n",
    "\n",
    "    n_unit, n_sub, _ = pw_ISC.shape\n",
    "\n",
    "    # SWB Permutations -> save the p-values(adj.)\n",
    "    stat = np.zeros((n_unit,n_perm))\n",
    "    p_val = np.zeros((n_unit,1))\n",
    "    for u in tqdm(range(n_unit)):\n",
    "        for p in range(n_perm):\n",
    "            b = bootstrap_pw_matrix(pw_ISC[u])\n",
    "            stat[u,p] = np.median(b)\n",
    "\n",
    "    pval = (stat<0).mean(axis=1)\n",
    "    pval_adj = fdr(pval, method='by')\n",
    "\n",
    "    # Save options\n",
    "    output_name = f\"pwISC_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{atlas}_SWB-pvalues(fdr).h5\"\n",
    "    output_fullpath = os.path.join(output_path, output_name)\n",
    "\n",
    "    # Save pvalues to a HDF5 file\n",
    "    with h5py.File(output_fullpath, 'w') as f:\n",
    "        f.create_dataset('pval', data=pval)\n",
    "        f.create_dataset('pval_adj', data=pval_adj)\n",
    "        f.create_dataset('controls', data=controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_file_pattern = f\"pwISC_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{{atlas}}_SWB-pvalues(fdr).h5\"\n",
    "bootstrap_path =  './eplink-p3/stats/'\n",
    "\n",
    "pwISC =\n",
    "sig = dict()\n",
    "\n",
    "for a, atlas in enumerate(atlases[1:]):\n",
    "    print(f'Loading atlas: {atlas}')\n",
    "    # Load ISCs\n",
    "    print('Loading ISCs ...')\n",
    "    _pw_ISC, subjects = load_pwISC(file_pattern.format(atlas=atlas,hemi='{hemi}'))\n",
    "    # Load pvalues\n",
    "    print('Loading pvalues ...')\n",
    "    fpath = os.path.join(bootstrap_path, bootstrap_file_pattern).format(atlas=atlas)\n",
    "    with h5py.File(fpath, 'r') as f:\n",
    "        pval_adj = f['pval_adj'][:]\n",
    "    print('done!')\n",
    "\n",
    "    # Set the significance threshold\n",
    "    sig[atlas] = pval_adj < 0.001  \n",
    "\n",
    "    # Patients and healthy controls indices\n",
    "    subjects = np.array(list(map(int, subjects)))\n",
    "    controls = subjects>5000\n",
    "    controls[np.isin(subjects, subjid_exclude)] = False\n",
    "    patients = subjects<5000\n",
    "    patients[np.isin(subjects, subjid_exclude)] = False\n",
    "\n",
    "    _pw_ISC[np.isnan(_pw_ISC)] = 0\n",
    "\n",
    "    # quick fix for the Desikan problem\n",
    "    if atlas == 'Desikan':\n",
    "        n_sub = _pw_ISC.shape[1]\n",
    "        # sig = np.concatenate(([False], sig[:35], [False], sig[35:]))\n",
    "        # pw_ISC = np.concatenate((np.zeros((1,n_sub,n_sub)), pw_ISC[:35], np.zeros((1,n_sub,n_sub)), pw_ISC[35:]))\n",
    "        sig[atlas] = np.concatenate((sig[atlas][:4], [False], sig[atlas][4:39], [False], sig[atlas][39:]))\n",
    "        _pw_ISC = np.concatenate((_pw_ISC[:4], np.zeros((1,n_sub,n_sub)), _pw_ISC[4:39], np.zeros((1,n_sub,n_sub)), _pw_ISC[39:]))\n",
    "\n",
    "    pw_ISC_cont = _pw_ISC[:,controls,:][:,:,controls]\n",
    "    pw_ISC_pat = _pw_ISC[:,patients,:][:,:,patients]\n",
    "\n",
    "    # Generate the surface maps\n",
    "    con_vec = np.vstack([vectorize_lt_matrix(pw_ISC_cont[u,:,:]) for u in range(pw_ISC_cont.shape[0])])\n",
    "    con_vec_med = np.median(con_vec, axis=1)\n",
    "\n",
    "    n_units = sig[atlas].shape[0]\n",
    "    sig[atlas][0] = False\n",
    "    sig[atlas][n_units//2] = False\n",
    "\n",
    "    con_vec_med[~sig[atlas]] = np.nan # mask the non-significant rois \n",
    "\n",
    "    output_path = os.path.join('..','surface_maps',dataset)\n",
    "    if atlas == 'none':\n",
    "        n_units = con_vec_med.shape[0]\n",
    "        con_vec_med = {'L': con_vec_med[:n_units//2], 'R': con_vec_med[n_units//2:]}\n",
    "        vertex2gii(con_vec_med, output_path, f'pwISC_desc-median_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{atlas}')\n",
    "    else:\n",
    "        atlasfile = os.path.join(atlases_path,f'{atlas}.32k.{{hemi}}.label.gii')\n",
    "        roi2gii(con_vec_med, atlasfile, output_path, f'pwISC_desc-median_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{atlas}')\n",
    "\n",
    "    median_ISC[atlas] = con_vec_med\n",
    "    pw_ISC[atlas] = _pw_ISC\n",
    "\n",
    "\n",
    "n_patients = patients.sum()\n",
    "n_controls = controls.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
