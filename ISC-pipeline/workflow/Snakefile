# ---- begin snakebids boilerplate ----------------------------------------------

import snakebids
from snakebids import bids, generate_inputs
from os.path import join
import pandas as pd


configfile: "config/snakebids.yml"


H_to_hemi = dict({"L": "lh", "R": "rh"})

# writes inputs_config.yml and updates config dict
inputs = snakebids.generate_inputs(
    bids_dir=config["fMRIprep_dir"].format(dataset=config["dataset"]),
    pybids_inputs=config["pybids_inputs"],
    pybids_config=["bids", "derivatives"],
    # derivatives=config["derivatives"],
    # participant_label=config["participant_label"],
    # exclude_participant_label=config["exclude_participant_label"],
    use_bids_inputs=True,
)


# this adds constraints to the bids naming
wildcard_constraints:
    **snakebids.get_wildcard_constraints(config["pybids_inputs"]),


# ---- end snakebids boilerplate ------------------------------------------------


# additional constraints for wildcards not defined from inputs
wildcard_constraints:
    desc="[a-zA-Z0-9]+",
    fwhm="[0-9]+",
    confounds="[0-9]+",
    surfname="white|pial|sphere.reg",
    volname="T1",


## Smoothing and denoising rules
rule smooth:
    input:
        nii=inputs["preproc_bold"].path,
        json=re.sub(".nii.gz", ".json", inputs["preproc_bold"].path),
    params:
        fwhm=lambda wildcards: float(wildcards.fwhm),
    output:
        nii=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="smoothed",
            fwhm="{fwhm}",
            suffix="bold.nii.gz",
            **inputs["preproc_bold"].wildcards,
        ),
        json=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="smoothed",
            fwhm="{fwhm}",
            suffix="bold.json",
            **inputs["preproc_bold"].wildcards,
        ),
    group:
        "subj"
    script:
        "scripts/smooth.py"


rule denoise:
    input:
        nii=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="smoothed",
            fwhm="{fwhm}",
            suffix="bold.nii.gz",
            **inputs["preproc_bold"].wildcards,
        ),
        json=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="smoothed",
            fwhm="{fwhm}",
            suffix="bold.json",
            **inputs["preproc_bold"].wildcards,
        ),
        confounds_tsv=inputs["confounds"].path,
        mask_nii=inputs["preproc_mask"].path,
    params:
        confounds_to_use=lambda wildcards: config["confounds"][
            int(wildcards.confounds_idx) - 1
        ]["regressors"],
        confounds_name=lambda wildcards: config["confounds"][
            int(wildcards.confounds_idx) - 1
        ]["name"],
        standardize=True,
        detrend=True,
        low_pass=False,
        high_pass=False,
    output:
        nii=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="denoised",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            suffix="bold.nii.gz",
            **inputs["preproc_bold"].wildcards,
        ),
        json=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="denoised",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            suffix="bold.json",
            **inputs["preproc_bold"].wildcards,
        ),
    group:
        "subj"
    script:
        "scripts/denoise.py"


# Dummy rules
rule just_denoise_all:
    input:
        denoised=expand(
            expand(
                rules.denoise.output.nii,
                zip,
                **inputs["preproc_bold"].zip_lists,
                allow_missing=True,
            ),
            dataset=config["dataset"],
            fwhm=config["fwhm"],
            confounds_idx=range(1, len(config["confounds"]) + 1),
        ),


rule smooth_denoise_all:
    input:
        # using the zip lists to expand over all scans, note use of the zip option in expand:
        denoised=expand(
            expand(
                bids(
                    root="results/{{dataset}}/denoised",
                    datatype="func",
                    desc="{{desc}}",
                    fwhm="{{fwhm}}",
                    confounds="{{confounds_idx}}",
                    suffix="bold.nii.gz",
                    **inputs["preproc_bold"].wildcards,
                ),
                zip,
                **inputs["preproc_bold"].zip_lists,
            ),
            dataset=config["dataset"],
            fwhm=config["fwhm"],
            confounds_idx=range(1, len(config["confounds"]) + 1),
            desc="denoised",
        ),


# default_target: True


## Generate midthickness rules
rule extract_from_tar:
    input:
        tar=config["freesurfer_tar"].format(
            dataset=config["dataset"], subject="{subject}"
        ),
    params:
        out_folder=config["freesurfer_root"].format(dataset=config["dataset"]),
        file_in_tar="sub-{subject}/{modality}/{filename}",
    output:
        filename=join(
            config["freesurfer"].format(
                dataset=config["dataset"], subject="{subject}"
            ),
            "{modality,surf|mri}",
            "{filename}",
        ),
    group:
        "participant1"
    shell:
        "mkdir -p {params.out_folder} && tar -C {params.out_folder} --extract --file={input.tar} {params.file_in_tar}"


def get_gifti_input(wildcards):
    if (
        wildcards.surfname == "pial"
    ):  # add .T1 to the name (since pial is a symlink to pial.T1) so can use if extracting from tar
        return join(
            config["freesurfer"].format(dataset=config["dataset"], subject="{subject}"),
            "surf",
            "{hemi}.{surfname}.T1".format(
                hemi=H_to_hemi[wildcards.hemi], surfname=wildcards.surfname
            ),
        )
    else:
        return join(
            config["freesurfer"].format(dataset=config["dataset"], subject="{subject}"),
            "surf",
            "{hemi}.{surfname}".format(
                hemi=H_to_hemi[wildcards.hemi], surfname=wildcards.surfname
            ),
        )


rule convert_to_gifti:
    input:
        get_gifti_input,
    output:
        bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            suffix="{surfname}.surf.gii",
            space="fsaverage",
        ),
    params:
        license=config["fs_license"],
    container:
        config["singularity_freesurfer"]
    log:
        "logs/{dataset}/convert_to_gifti/sub-{subject}_{hemi}_{surfname}.log",
    group:
        "participant1"
    shell:
        "FS_LICENSE={params.license} mris_convert {input} {output} &> {log}"


rule convert_to_nifti:
    input:
        join(config["freesurfer"], "mri", "{volname}.mgz"),
    output:
        bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            suffix="{volname}.nii.gz",
        ),
    params:
        license=config["fs_license"],
    container:
        config["singularity_freesurfer"]
    log:
        "logs/{dataset}/convert_to_nifti/sub-{subject}_{volname}.log",
    group:
        "participant1"
    shell:
        "FS_LICENSE={params.license} mri_convert {input} {output} &> {log}"


rule get_tkr2scanner:
    input:
        t1=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            suffix="T1.nii.gz",
        ),
    output:
        tkr2scanner=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            suffix="tkr2scanner.xfm",
        ),
    params:
        license=config["fs_license"],
    container:
        config["singularity_freesurfer"]
    log:
        "logs/{dataset}/get_tkr2scanner/sub-{subject}.log",
    group:
        "participant1"
    shell:
        "FS_LICENSE={params.license} mri_info {input.t1} --tkr2scanner > {output.tkr2scanner} 2> {log}"


rule apply_surf_tkr2scanner:
    input:
        surf=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            suffix="{surfname}.surf.gii",
            space="fsaverage",
        ),
        tkr2scanner=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            suffix="tkr2scanner.xfm",
        ),
    output:
        surf=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            space="native",
            suffix="{surfname}.surf.gii",
        ),
    threads: 8
    container:
        config["singularity_connectome_workbench"]
    log:
        "logs/{dataset}/apply_surf_tkr2scanner/sub-{subject}_{hemi}_{surfname}.log",
    group:
        "participant1"
    shell:
        "wb_command -surface-apply-affine {input.surf} {input.tkr2scanner} {output.surf} &> {log}"


rule gen_midthickness:
    input:
        white=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            suffix="white.surf.gii",
            space="{space}",
        ),
        pial=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            suffix="pial.surf.gii",
            space="{space}",
        ),
    output:
        midthickness=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            suffix="midthickness.surf.gii",
            space="{space}",
        ),
    container:
        config["singularity_connectome_workbench"]
    threads: 8
    log:
        "logs/{dataset}/gen_midthickness/sub-{subject}_{hemi}_{space}.log",
    group:
        "participant1"
    shell:
        "wb_command -surface-average {output.midthickness} -surf {input.white} -surf {input.pial} &> {log}"


# Dummy rule
rule gen_midthickness_all:
    input:
        midthickness=expand(
            bids(
                root="results/{dataset}/midthickness",
                subject="{subject}",
                hemi="{hemi}",
                space="native",
                suffix="midthickness.surf.gii",
            ),
            dataset=config["dataset"],
            subject=inputs.subjects,
            hemi=config["hemi"],
        ),


# default_target: True


## Resample to surface rules
rule vol2surf:
    input:
        bold=bids(
            root="results/{dataset}/denoised",
            datatype="func",
            desc="denoised",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            space="T1w",
            subject="{subject}",
            task="{task}",
            run="{run}",
            suffix="bold.nii.gz",
        ),
        midthickness=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            space="native",
            suffix="midthickness.surf.gii",
        ),
        white=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            space="native",
            suffix="white.surf.gii",
        ),
        pial=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            space="native",
            suffix="pial.surf.gii",
        ),
    output:
        bids(
            root="results/{dataset}/resampled2fsLR",
            datatype="func",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            desc="denoised",
            hemi="{hemi}",
            space="native",
            suffix="bold.func.gii",
            subject="{subject}",
            task="{task}",
            run="{run}",
        ),
    shell:
        "wb_command -volume-to-surface-mapping {input.bold} {input.midthickness} {output} -ribbon-constrained {input.white} {input.pial}"


rule metric_resample:
    input:
        vol2surf_out=rules.vol2surf.output,
        curr_sphere=bids(
            root="results/{dataset}/midthickness",
            subject="{subject}",
            hemi="{hemi}",
            space="fsaverage",
            suffix="sphere.reg.surf.gii",
        ),
        target_sphere="resources/standard_mesh_atlases/resample_fsaverage/fs_LR-deformed_to-fsaverage.{hemi}.sphere.32k_fs_LR.surf.gii",
    output:
        bids(
            root="results/{dataset}/resampled2fsLR/32k_space_surfaces",
            datatype="func",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            desc="denoised",
            hemi="{hemi}",
            space="fsLR_den-32k",
            suffix="bold.func.gii",
            subject="{subject}",
            task="{task}",
            run="{run}",
        ),
    shell:
        "wb_command -metric-resample {input.vol2surf_out} {input.curr_sphere} {input.target_sphere} BARYCENTRIC {output}"


# Dummy rule
rule resample2fsLR_all:
    input:
        fsLR=expand(
            expand(
                rules.metric_resample.output,
                zip,
                **inputs["preproc_bold"].zip_lists,
                allow_missing=True,
            ),
            dataset=config["dataset"],
            fwhm=config["fwhm"],
            confounds_idx=range(1, len(config["confounds"]) + 1),
            hemi=config["hemi"],
        ),


# default_target: True


## Temporal resampling
rule temp_resample:
    input:
        func=rules.metric_resample.output,
    params:
        original_TR=config["original_TR"],
        target_TR=config["target_TR"],
    output:
        bids(
            root="results/{dataset}/temporalResampled",
            datatype="func",
            fwhm="{fwhm}",
            confounds="{confounds_idx}",
            desc="denoised",
            hemi="{hemi}",
            space="fsLR_den-32k",
            suffix="bold.func.gii",
            subject="{subject}",
            task="{task}",
            run="{run}",
        ),
    script:
        "scripts/tempResample.py"


# Dummy rule
rule temp_resample_all:
    input:
        fsLR=expand(
            expand(
                rules.temp_resample.output,
                zip,
                **inputs["preproc_bold"].zip_lists,
                allow_missing=True,
            ),
            dataset=config["dataset"],
            fwhm=config["fwhm"],
            confounds_idx=range(1, len(config["confounds"]) + 1),
            hemi=config["hemi"],
        ),


# default_target: True


## Parcellate
rule parcellate:
    input:
        func=bids(
            root="results/{dataset}/{source}",
            datatype="func",
            fwhm=0,
            confounds="{confounds_idx}",
            desc="denoised",
            hemi="{hemi}",
            space="fsLR_den-32k",
            suffix="bold.func.gii",
            subject="{subject}",
            task="{task}",
            run="{run}",
        ),
        atlas="resources/atlases_fsLR_32K/cleaned_versions/{atlas}.32k.{hemi}.label.gii",
    output:
        h5=bids(
            root="results/{dataset}/{source}-parcellated/{atlas}",
            datatype="func",
            subject="{subject}",
            task="{task}",
            run="{run}",
            confounds="{confounds_idx}",
            hemi="{hemi}",
            suffix="bold.func.h5",
        ),
    script:
        "scripts/surfParcellate.py"


# Dummy rule
rule parcellate_resampled_all:
    input:
        parcellate=expand(
            expand(
                rules.parcellate.output.h5,
                zip,
                **inputs["preproc_bold"].zip_lists,
                allow_missing=True,
            ),
            dataset=config["dataset"],
            source="temporalResampled",
            confounds_idx=range(1, len(config["confounds"]) + 1),
            hemi=config["hemi"],
            atlas=config["parcellations"],
        ),


    # default_target: True


rule parcellate_original_all:
    input:
        parcellate=expand(
            expand(
                rules.parcellate.output.h5,
                zip,
                **inputs["preproc_bold"].zip_lists,
                allow_missing=True,
            ),
            dataset=config["dataset"],
            source="resampled2fsLR/32k_space_surfaces",
            confounds_idx=range(1, len(config["confounds"]) + 1),
            hemi=config["hemi"],
            atlas=config["parcellations"],
        ),


    # default_target: True

## Functional Connectivity
rule pearson_fc: # Uses original data (no temporal resampling)
    input:
        func = expand(
            bids(root="results/{dataset}/resampled2fsLR/32k_space_surfaces-parcellated/{atlas}",
                datatype="func",
                subject="{subject}",
                task="{task}",
                run="{run}",
                confounds="{confounds_idx}",
                hemi="{hemi}",
                suffix="bold.func.h5"
            ),
            hemi=config["hemi"],
            allow_missing=True
        )
    params:
        n_dummies=config['n_dummies'],
        n_volumes=config['n_volumes'],
    output:
        h5 = bids(root="results/{dataset}/FC/{atlas}",
            datatype="func",
            subject="{subject}",
            task="{task}",
            run="{run}",
            confounds="{confounds_idx}",
            suffix="pearson.h5"
        )
    script:
        "scripts/pearson_FC.py"

# Dummy rules
rule fc_all: 
    input:
        fc = expand(
                expand(rules.pearson_fc.output.h5,
                    zip,
                    **inputs["preproc_bold"].zip_lists,
                    allow_missing=True
                ),
                dataset = config["dataset"],
                confounds_idx=range(1, len(config["confounds"]) + 1),
                atlas=config["parcellations"]
            )
    
    default_target: True


## pair-wise ISC
def subject_id_filter(bidscomp):
    zip_lists = bidscomp.zip_lists
    # filter for control subjects ids > 5000
    ind = [i for i, s in enumerate(zip_lists["subject"]) if int(s) > 5000]
    filtered_zip_lists = dict()
    for k, v in zip_lists.items():
        filtered_zip_lists[k] = [v[i] for i in ind]

    filtered = snakebids.BidsComponent(
        "filtered_by_subject", bidscomp.path, filtered_zip_lists
    )
    return filtered


def ISC_gen_inputs(wildcards):
    # Determine the inputs source based on temporal resampling
    if wildcards.resampled == "Y":
        source = "temporalResampled"
    else:
        source = "resampled2fsLR/32k_space_surfaces"

    # Determine the subject range based on the group selected (controls, patients, all)
    if hasattr(wildcards, "group"):
        filtered = subject_id_filter(inputs["preproc_bold"])  # select controls
    else:
        filtered = inputs["preproc_bold"]  # proceed with all

    # Different inputs are required for vertex level and parcellated
    if wildcards.atlas == "none":  # Vertex level
        ins = expand(
            bids(
                root=f"results/{wildcards.dataset}/{source}",
                datatype="func",
                subject="{subject}",
                task="{task}",  # It's important for the task to be filtered later not forcing all subjects having all tasks
                run="{run}",
                fwhm="{fwhm}",
                space="fsLR_den-32k",
                desc="denoised",
                confounds=wildcards.confounds_idx,
                hemi=wildcards.hemi,
                suffix="bold.func.gii",
            ),
            zip,
            **filtered.zip_lists,
            allow_missing=True,
        )
    # Parcellated
    # It's important for the task to be filtered later not forcing all subjects having all tasks
    else:
        ins = expand(
            bids(
                root=f"results/{wildcards.dataset}/{source}-parcellated/{wildcards.atlas}",
                datatype="func",
                subject="{subject}",
                task="{task}",
                run="{run}",
                confounds=wildcards.confounds_idx,
                hemi=wildcards.hemi,
                suffix="bold.func.h5",
            ),
            zip,
            **filtered.zip_lists,
            allow_missing=True,
        )
    return ins


rule pw_ISC:
    input:
        func=ISC_gen_inputs,
    params:
        n_dummies=config['n_dummies'],
        n_volumes=config['n_volumes'],
    output:
        h5="results/{dataset}/pwISC/pwISC_task-{task}_hemi-{hemi}_fwhm-{fwhm}_confounds-{confounds_idx}_resampled-{resampled}_atlas-{atlas}.h5",
    script:
        "scripts/pwISC.py"


# Dumy rule
# atlas = none would just use vertices as units
rule pw_ISC_movie:
    input:
        expand(
            expand(
                rules.pw_ISC.output.h5,
                dataset=config["dataset"],
                task=config["movie_name"],
                hemi=config["hemi"],
                fwhm="{fwhm}",
                confounds_idx=1,
                resampled=config["resampling"],
                atlas="{atlas}",
            ),
            zip,
            fwhm=[0] * len(config["parcellations"]) + config["fwhm"],
            atlas=config["parcellations"] + ["none"] * len(config["fwhm"]),
        ),


# default_target: True


## leave-one-out ISC
rule loo_ISC:
    input:
        func=ISC_gen_inputs,
    params:
        n_dummies=config['n_dummies'],
        n_volumes=config['n_volumes'],
    output:
        h5="results/{dataset}/looISC/{group}/looISC_task-{task}_hemi-{hemi}_fwhm-{fwhm}_confounds-{confounds_idx}_resampled-{resampled}_atlas-{atlas}.h5",
    script:
        "scripts/looISC.py"


# Dumy rule
# atlas = none would just use vertices as units
rule loo_ISC_movie:
    input:
        expand(
            expand(
                rules.loo_ISC.output.h5,
                dataset=config["dataset"],
                task=config["movie_name"],
                group="control",
                hemi=config["hemi"],
                fwhm="{fwhm}",
                confounds_idx=1,
                resampled=config["resampling"],
                atlas="{atlas}",
            ),
            zip,
            fwhm=[0] * len(config["parcellations"]) + config["fwhm"],
            atlas=config["parcellations"] + ["none"] * len(config["fwhm"]),
        ),


# default_target: True


rule ISC_all:
    input:
        loo = rules.loo_ISC_movie.input,
        pw = rules.pw_ISC_movie.input
    
    # default_target: True