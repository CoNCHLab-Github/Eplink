{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import importlib\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.Surf import roi2gii, vertex2gii\n",
    "import utils.Vis\n",
    "importlib.reload(utils.Vis)\n",
    "from utils.Vis import plot_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config parameters\n",
    "root = \"/home/ali/graham-akhanf/EpLink/Eplink\"\n",
    "results_path = os.path.join(root,\"ISC-pipeline\",\"results\",\"{dataset}\",\"looISC\",\"control\")\n",
    "atlases_path = os.path.join('.','ISC-pipeline','resources','atlases_fsLR_32K')\n",
    "\n",
    "atlases = ['none', 'Glasser_2016', 'Desikan', 'Schaefer2018_17Networks_400', 'Yan2023_17Networks_400']\n",
    "atlases_aliases = ['Vertex', 'Glasser 2016', 'Desikan', 'Schaefer 2018', 'Yan 2023']\n",
    "\n",
    "atlases = ['none', 'Glasser_2016', 'Desikan', 'Yan2023_17Networks_400']\n",
    "atlases_aliases = ['Vertex', 'Glasser 2016', 'Desikan', 'Yan 2023']\n",
    "\n",
    "datasets = [\"eplink-p2\", \"eplink-p3\"]\n",
    "task_dataset = {'eplink-p2': 'hitchcock', 'eplink-p3': 'movie'}\n",
    "source_dataset = {'eplink-p2': 'resampled2fsLR/32k_space_surfaces-parcellated',\\\n",
    "                  'eplink-p3': 'temporalResampled-parcellated'}\n",
    "resampled_dataset = {'eplink-p2': 'N', 'eplink-p3': 'Y'}\n",
    "\n",
    "fwhm = 0\n",
    "confounds_idx = 1\n",
    "target_volumes = 240\n",
    "\n",
    "output_path = os.path.join(root, 'results', 'phases_combined', 'looISC')\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "# file_pattern = \"looISC_task-{task}_hemi-{{hemi}}_fwhm-{fwhm}_confounds-{confounds_idx}_resampled-{resampled}_atlas-{atlas}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_dimensions(filepath):\n",
    "    \"\"\"Extract dimensions of data from h5 data.\"\"\"\n",
    "    # Check file extension\n",
    "    ext = os.path.splitext(filepath)[-1]\n",
    "    \n",
    "    # h5 file\n",
    "    if ext == '.h5': \n",
    "        # Open the HDF5 file\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Load the parcellated data\n",
    "            data = f['parcellated_data'][:]\n",
    "            # Return data dimensions\n",
    "            n_rois, n_vol = data.shape\n",
    "    \n",
    "    # gii file\n",
    "    elif ext == '.gii':\n",
    "        # Load the GIFTI file by nibabel\n",
    "        gii = nib.load(filepath)\n",
    "        # Return data dimensions\n",
    "        n_rois = gii.darrays[0].data.shape[0]\n",
    "        n_vol = len(gii.darrays)\n",
    "    \n",
    "    return n_rois, n_vol\n",
    "\n",
    "def load_HDF(filepath, n_vols):\n",
    "    \"\"\"Load data stored in the HDF files given the runs dataframe.\"\"\"\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "            # Load the parcellated data\n",
    "            data = f['parcellated_data'][:]\n",
    "            data = data[:,:n_vols] # Ignoring excessive volumes\n",
    "    # Returns data with shape ROI x Time\n",
    "    return data\n",
    "\n",
    "def load_gii(filepath, n_vols):\n",
    "    func_gii = nib.load(filepath)\n",
    "    data = np.vstack([darray.data for darray in func_gii.darrays[:n_vols]]).T\n",
    "    # Returns data with shape Vertex x Time\n",
    "    return data\n",
    "\n",
    "def load_runs(runs_df, n_vols):\n",
    "    # Loading only the last run\n",
    "    fp = runs_df['full_path'].iloc[-1]\n",
    "    # Checking file extension\n",
    "    ext = os.path.splitext(fp)[-1]\n",
    "\n",
    "    # h5 file\n",
    "    if ext == '.h5':\n",
    "        data = load_HDF(fp, n_vols)\n",
    "    # gii file\n",
    "    elif ext == '.gii':\n",
    "        data = load_gii(fp, n_vols)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_info(filepath):\n",
    "    \"\"\"Extract subject, hemi, task, run, fwhm, confounds,\n",
    "      rois, and number of volumes from a given file.\"\"\"\n",
    "    # Get file name\n",
    "    basename = os.path.basename(filepath)\n",
    "    # Split file name by '_'\n",
    "    parts = basename.split('_')\n",
    "    # Info dictionary\n",
    "    info = {}\n",
    "    # Parse file name\n",
    "    for part in parts:\n",
    "        info['full_path'] = filepath\n",
    "\n",
    "        if part.startswith('sub-'):\n",
    "            info['subject'] = part.split('sub-')[1]\n",
    "        elif part.startswith('hemi-'):\n",
    "            info['hemi'] = part.split('hemi-')[1]\n",
    "        elif part.startswith('task-'):\n",
    "            info['task'] = part.split('task-')[1]\n",
    "        elif part.startswith('run-'):\n",
    "            info['run'] = part.split('run-')[1]\n",
    "        elif part.startswith('fwhm-'):\n",
    "            info['fwhm'] = part.split('fwhm-')[1]\n",
    "        elif part.startswith('confounds-'):\n",
    "            info['confounds'] = part.split('confounds-')[1].split('_')[0]  # Assuming confounds is the last part before the file extension\n",
    "\n",
    "    # Get dimensions for stored data    \n",
    "    info['n_roi'], info['n_vol'] = get_dimensions(filepath)\n",
    "    \n",
    "    return info\n",
    "\n",
    "def build_dataframe(directory, pattern=None):\n",
    "    \"\"\"Build a dataframe from files in the directory and its subdirectories.\"\"\"\n",
    "    # Default Pattern for search\n",
    "    if pattern == None:\n",
    "        subject, hemi, task, run, fwhm, confounds = ('*', '*', '*', '*', 0, 1)\n",
    "        pattern = f\"sub-{subject}_hemi-{hemi}_task-{task}_run-{run}_space-fsLR_den-32k_desc-denoised_fwhm-{fwhm}_confounds-{confounds}_atlas-glasser.h5\"\n",
    "    \n",
    "    pattern = os.path.join(directory, 'sub-*', 'func', pattern)\n",
    "    # Get all files matching the pattern\n",
    "    files = glob.glob(pattern, recursive=True)\n",
    "    # Building the dataframe\n",
    "    df = pd.DataFrame([get_info(file) for file in files])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glasser_2016\n",
      "eplink-p2\n",
      "eplink-p3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/ISC-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ali/anaconda3/envs/ISC-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "181.0\n",
      "Desikan\n",
      "eplink-p2\n",
      "eplink-p3\n",
      "72\n",
      "36.0\n",
      "Yan2023_17Networks_400\n",
      "eplink-p2\n",
      "eplink-p3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/ISC-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ali/anaconda3/envs/ISC-env/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802\n",
      "401.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,len(atlases)):\n",
    "    print(atlases[i])\n",
    "    df_comb = pd.DataFrame()\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        data_path = os.path.join(root, 'ISC-pipeline', 'results', f'{dataset}', f'{source_dataset[dataset]}', f'{atlases[i]}')\n",
    "        # subjects_path = glob.glob(os.path.join(data_path, 'sub-*'))\n",
    "        # subjects = [int(os.path.basename(p)[4:]) for p in subjects_path]\n",
    "        # subjects.sort()\n",
    "\n",
    "        subject, hemi, task, run, fwhm, confounds = ('*', '*', task_dataset[dataset], '*', 0, 1)\n",
    "        datafile_pattern = f\"sub-{subject}_task-{task}_run-{run}_hemi-{hemi}_confounds-{confounds}_bold.func.h5\"\n",
    "\n",
    "        df = build_dataframe(data_path, datafile_pattern)\n",
    "        # Exclude files with less volumes\n",
    "        df = df[df[\"n_vol\"] >= target_volumes].reset_index(drop=True)\n",
    "\n",
    "        df_comb = pd.concat((df_comb, df))\n",
    "\n",
    "    # Sort files \n",
    "    df_comb = df_comb.sort_values(by=['subject', 'task', 'run'], ascending=[True]*3).reset_index()\n",
    "    # Ignore some columns\n",
    "    #['subject', 'task', 'run', 'hemi', 'fwhm', 'confounds', 'n_vol', 'full_path']\n",
    "    df_comb = df_comb[['subject', 'task', 'run', 'hemi', 'n_vol', 'full_path']]\n",
    "\n",
    "    # Get unique subjects\n",
    "    subjects = df_comb['subject'].unique()\n",
    "    controls = [s for s in subjects if int(s) > 5000]\n",
    "    patients = [s for s in subjects if int(s) < 5000]\n",
    "    n_subject = len(subjects)\n",
    "\n",
    "    # Load files data\n",
    "    data = []\n",
    "    for subj in controls:\n",
    "        # Filter subject files \n",
    "        df_s = df_comb[df_comb['subject'] == subj]\n",
    "        df_s_L = df_s[df_s['hemi'] == 'L']\n",
    "        df_s_R = df_s[df_s['hemi'] == 'R']\n",
    "        # Load runs\n",
    "        d_L = load_runs(df_s_L, target_volumes)\n",
    "        d_R = load_runs(df_s_R, target_volumes)\n",
    "        data.append(np.concatenate((d_L,d_R),axis=0))\n",
    "\n",
    "    # Stacking subjects data (Subject, Unit, Time)\n",
    "    data = np.stack(data)\n",
    "\n",
    "    n_controls, n_unit, n_vols = data.shape\n",
    "    output_name = f\"looISC_hemi-{{hemi}}_fwhm-{fwhm}_confounds-{confounds_idx}_atlas-{atlases[i]}.h5\"\n",
    "    output_fullpath = os.path.join(output_path, output_name)\n",
    "\n",
    "    # Sum subjects data (Unit, Time)\n",
    "    data_sum = data.sum(axis=0)\n",
    "    # Output will be shaped as (Subject, Unit)\n",
    "    loo_ISC = np.zeros((n_controls, n_unit))\n",
    "    for i in range(n_controls):\n",
    "        for j in range(n_unit):\n",
    "            sum_ts = data_sum[j,:].reshape((1,-1))\n",
    "            sub_ts = data[i,j,:].reshape((1,-1))\n",
    "            c = np.corrcoef(sub_ts, sum_ts-sub_ts)\n",
    "            loo_ISC[i,j] = c[0,1]\n",
    "\n",
    "    loo_ISC_dict = {'L': loo_ISC[:,:int(n_unit/2)],\n",
    "                    'R': loo_ISC[:,int(n_unit/2):]}\n",
    "    # Save ISCs as HDF5 file\n",
    "    for hemi in ['L', 'R']:\n",
    "        with h5py.File(output_fullpath.format(hemi=hemi), 'w') as f:\n",
    "            f.create_dataset('loo_ISC', data=loo_ISC_dict[f'{hemi}'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISC-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
